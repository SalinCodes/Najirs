{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>्तव्य ज्यान प्रकरण नं भवितव्य ठहर्न ज्यान लिने...</td>\n",
       "      <td>[[-0.023295283, 0.04019555, 0.22646093, 0.1248...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>्प्रेषण प्रकरण नं रिट निवेदकलाई श्रम कार्यालयल...</td>\n",
       "      <td>[[-0.18046029, 0.052735027, 0.23040603, 0.1659...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>्प्रेषण परमादेशको आदेश जारी गरिपाउँ प्रनं धेरै...</td>\n",
       "      <td>[[-0.2502869, -0.0183751, 0.117184184, 0.09710...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>्प्रेषण प्रकरण नं जिल्ला धनुषा जनकपुर नगर पंचा...</td>\n",
       "      <td>[[-0.2658006, -0.10030958, 0.21205233, 0.08202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>्दीप्रत्यक्षीकरण प्रकरण नं सार्वजनिक सुरक्षा ऐ...</td>\n",
       "      <td>[[-0.11624412, -0.095075056, 0.28900784, 0.041...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text  \\\n",
       "0  ्तव्य ज्यान प्रकरण नं भवितव्य ठहर्न ज्यान लिने...   \n",
       "1  ्प्रेषण प्रकरण नं रिट निवेदकलाई श्रम कार्यालयल...   \n",
       "2  ्प्रेषण परमादेशको आदेश जारी गरिपाउँ प्रनं धेरै...   \n",
       "3  ्प्रेषण प्रकरण नं जिल्ला धनुषा जनकपुर नगर पंचा...   \n",
       "4  ्दीप्रत्यक्षीकरण प्रकरण नं सार्वजनिक सुरक्षा ऐ...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [[-0.023295283, 0.04019555, 0.22646093, 0.1248...  \n",
       "1  [[-0.18046029, 0.052735027, 0.23040603, 0.1659...  \n",
       "2  [[-0.2502869, -0.0183751, 0.117184184, 0.09710...  \n",
       "3  [[-0.2658006, -0.10030958, 0.21205233, 0.08202...  \n",
       "4  [[-0.11624412, -0.095075056, 0.28900784, 0.041...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the dataframes\n",
    "nepberta_df = pd.read_pickle(\"D:\\\\Python\\\\ML-Projects\\\\Intern-Synapse\\\\TextVectorization\\\\najirs_NepBERTa.pkl\")\n",
    "bm25_df = pd.read_pickle('D:\\\\Python\\\\ML-Projects\\\\Intern-Synapse\\\\TextVectorization\\\\najirs_BM25.pkl')\n",
    "\n",
    "nepberta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9355, 2) (9355, 68328)\n",
      "(9355, 768)\n"
     ]
    }
   ],
   "source": [
    "# Check initial shapes\n",
    "print(nepberta_df.shape, bm25_df.shape)\n",
    "\n",
    "# Preprocess: Convert embeddings into single flat lists\n",
    "def flatten_embedding(embedding):\n",
    "    # Flatten the nested list entirely if it exists\n",
    "    if isinstance(embedding, list) and len(embedding) > 0 and isinstance(embedding[0], list):\n",
    "        return np.array(embedding[0])\n",
    "    return np.array(embedding)\n",
    "\n",
    "# Apply flattening\n",
    "nepberta_df[\"flattened_embeddings\"] = nepberta_df[\"embeddings\"].apply(flatten_embedding)\n",
    "\n",
    "# Expand the flattened embeddings into separate columns\n",
    "expanded_embeddings = np.vstack(nepberta_df[\"flattened_embeddings\"].values)\n",
    "\n",
    "# Create a new DataFrame with embeddings\n",
    "embedding_df = pd.DataFrame(expanded_embeddings)\n",
    "\n",
    "# Verify shape\n",
    "print(embedding_df.shape)  # Should be (9355, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepberta_array = embedding_df.values.astype(np.float64)\n",
    "bm25_vectors = bm25_df.values          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting With The IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "\n",
    "#Initialize config parser\n",
    "config = ConfigParser()\n",
    "config.read(\"D:\\\\Python\\\\ML-Projects\\\\Intern-Synapse\\\\TextVectorization\\\\config_vector.ini\")\n",
    "\n",
    "#Accessing database details\n",
    "db_config = config['database']\n",
    "username = db_config['username']\n",
    "pwd = db_config['password']\n",
    "hostname = db_config['hostname']\n",
    "port_id = int(db_config['port_id'])\n",
    "database = db_config['database']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14204\\1808304932.py:13: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>data</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>१००३८</td>\n",
       "      <td>&lt;div class=\"col-md-8 para-sections\"&gt;\\n&lt;div id=...</td>\n",
       "      <td>{'body': ['न्या.डा.आनन्दमोहन भट्टराई : न्याय प...</td>\n",
       "      <td>२०७४-१२-१९</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>४६१५</td>\n",
       "      <td>&lt;div class=\"col-md-8 para-sections\"&gt;\\n&lt;div id=...</td>\n",
       "      <td>{'body': ['न्या.गजेन्द्रकेशरी वास्तोला', '१.  ...</td>\n",
       "      <td>२०४९-०९-०८</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>११२१</td>\n",
       "      <td>&lt;div class=\"col-md-8 para-sections\"&gt;\\n&lt;div id=...</td>\n",
       "      <td>{'body': ['न्या. धनेन्द्रबहादुर सिंह : प्रस्तु...</td>\n",
       "      <td>२०३५-०१-१२</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>२४०६</td>\n",
       "      <td>&lt;div class=\"col-md-8 para-sections\"&gt;\\n&lt;div id=...</td>\n",
       "      <td>{'body': ['न्या.बब्बरप्रसाद सिंहः नेपालको संवि...</td>\n",
       "      <td>२०४२-०५-१९</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>३१४४</td>\n",
       "      <td>&lt;div class=\"col-md-8 para-sections\"&gt;\\n&lt;div id=...</td>\n",
       "      <td>{'body': ['न्या.पृथ्वी बहादुर सिंहः नेपालको सं...</td>\n",
       "      <td>२०४४-०२-२५</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                            content  \\\n",
       "0  १००३८  <div class=\"col-md-8 para-sections\">\\n<div id=...   \n",
       "1   ४६१५  <div class=\"col-md-8 para-sections\">\\n<div id=...   \n",
       "2   ११२१  <div class=\"col-md-8 para-sections\">\\n<div id=...   \n",
       "3   २४०६  <div class=\"col-md-8 para-sections\">\\n<div id=...   \n",
       "4   ३१४४  <div class=\"col-md-8 para-sections\">\\n<div id=...   \n",
       "\n",
       "                                                data        date  \n",
       "0  {'body': ['न्या.डा.आनन्दमोहन भट्टराई : न्याय प...  २०७४-१२-१९  \n",
       "1  {'body': ['न्या.गजेन्द्रकेशरी वास्तोला', '१.  ...  २०४९-०९-०८  \n",
       "2  {'body': ['न्या. धनेन्द्रबहादुर सिंह : प्रस्तु...  २०३५-०१-१२  \n",
       "3  {'body': ['न्या.बब्बरप्रसाद सिंहः नेपालको संवि...  २०४२-०५-१९  \n",
       "4  {'body': ['न्या.पृथ्वी बहादुर सिंहः नेपालको सं...  २०४४-०२-२५  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating SQLAlchemy engine\n",
    "from sqlalchemy import create_engine, Column, String\n",
    "engine = create_engine(f'postgresql://{username}:{pwd}@{hostname}:{port_id}/{database}')\n",
    "\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "#Creating a session\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "#Defining ORM model\n",
    "Base = declarative_base()\n",
    "\n",
    "class Najirs(Base):\n",
    "    __tablename__ = \"najirs_v4\"\n",
    "\n",
    "    id = Column(String, primary_key=True)\n",
    "    content = Column(String)\n",
    "    date = Column(String)\n",
    "    data = Column(String)\n",
    "\n",
    "#Query data safely\n",
    "results = session.query(Najirs.id, Najirs.content, Najirs.data, Najirs.date).all()\n",
    "df = pd.DataFrame(results, columns= [\"id\", \"content\", \"data\", \"date\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 572 rows with empty 'principles' key.\n"
     ]
    }
   ],
   "source": [
    "df.drop(columns=['content', 'date'], inplace=True)\n",
    "\n",
    "def is_principles_empty(content):\n",
    "    return 'principles' not in content or not content['principles']\n",
    "\n",
    "#Removing rows with empty 'principles'\n",
    "empty_principles_mask = df['data'].apply(is_principles_empty)\n",
    "empty_principles_count = empty_principles_mask.sum()\n",
    "df = df[~empty_principles_mask]\n",
    "print(f\"Removed {empty_principles_count} rows with empty 'principles' key.\")\n",
    "\n",
    "df.drop(columns=['data'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (9355, 1)\n",
      "      id\n",
      "0  १००३८\n",
      "1   ४६१५\n",
      "2   ११२१\n",
      "3   २४०६\n",
      "4   ३१४४\n"
     ]
    }
   ],
   "source": [
    "df['id'] = df['id'].astype(str)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Convert your bm25_vectors into a numpy array if it's not already\n",
    "bm25_vectors_array = np.array(bm25_vectors, dtype=np.float64)\n",
    "\n",
    "# Convert the numpy array to a sparse CSR matrix\n",
    "bm25_vectors_sparse = csr_matrix(bm25_vectors_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and added records from 0 to 999\n",
      "Processed and added records from 1000 to 1999\n",
      "Processed and added records from 2000 to 2999\n",
      "Processed and added records from 3000 to 3999\n",
      "Processed and added records from 4000 to 4999\n",
      "Processed and added records from 5000 to 5999\n",
      "Processed and added records from 6000 to 6999\n",
      "Processed and added records from 7000 to 7999\n",
      "Processed and added records from 8000 to 8820\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "similarity_df = pd.DataFrame(columns=[\"document_id\", \"similar_document_ids\"])\n",
    "\n",
    "def get_top_similar_documents(query_id, top_n=10, weight_nepberta=0.6, weight_bm25=0.4, base_threshold=0.4, min_threshold = 0.3):\n",
    "    global queries_with_few_similar, all_similarity_scores\n",
    "    try:\n",
    "        query_index = df.index[df['id'] == query_id].tolist()[0] \n",
    "        print(f\"Query ID: {query_id}, Corresponding Index: {query_index}\")\n",
    "        \n",
    "        query_vector = np.array(nepberta_array[query_index])\n",
    "        #Compute cosine similarity for NepBERTa\n",
    "        nepberta_sim = cosine_similarity(query_vector.reshape(1, -1), nepberta_array).flatten()\n",
    "        \n",
    "        #Compute cosine similarity for BM25 using sparse vectors\n",
    "        query_bm25_vector = bm25_vectors_sparse[query_index]\n",
    "        bm25_sim = query_bm25_vector.dot(bm25_vectors_sparse.T).toarray().flatten()\n",
    "\n",
    "        #Normalize BM25 values to [-1, 1]\n",
    "        bm25_sim_normalized = 2 * (bm25_sim / np.max(bm25_sim)) - 1\n",
    "        \n",
    "        #Ensembling similarity (weighted average)\n",
    "        ensemble_sim = (weight_nepberta * nepberta_sim) + (weight_bm25 * bm25_sim_normalized)\n",
    "        \n",
    "        # Apply dynamic threshold adjustment\n",
    "        threshold = base_threshold\n",
    "        adjustment = 0.02\n",
    "        iteration_count = 0\n",
    "        max_iterations = 10\n",
    "\n",
    "        while len(ensemble_sim[ensemble_sim >= threshold]) < 5 and iteration_count < max_iterations:\n",
    "            threshold -= adjustment\n",
    "                \n",
    "            #Ensure threshold doesn't go below the minimum threshold\n",
    "            if threshold < min_threshold:\n",
    "                threshold = min_threshold\n",
    "                break\n",
    "                \n",
    "            iteration_count += 1\n",
    "        \n",
    "        #Filter based on the adjusted threshold\n",
    "        valid_indices = np.where(ensemble_sim >= threshold)[0]\n",
    "        \n",
    "        #Exclude the query index itself\n",
    "        valid_indices = valid_indices[valid_indices != query_index]\n",
    "        \n",
    "        #Sort by similarity score and get top_n results\n",
    "        top_indices = valid_indices[np.argsort(-ensemble_sim[valid_indices])]\n",
    "        top_indices = top_indices[:top_n] if len(top_indices) >= top_n else top_indices\n",
    "        \n",
    "        #Map indices to document IDs\n",
    "        top_similar_ids = [df.iloc[idx]['id'] for idx in top_indices]\n",
    "        \n",
    "        #Return the top similar document IDs\n",
    "        return top_similar_ids\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query_id {query_id}: {e}\")\n",
    "        return []\n",
    "\n",
    "def populate_similarity_dataframe():\n",
    "    global similarity_df\n",
    "    \n",
    "    for i in range(0, 8821, 1000):\n",
    "\n",
    "        end_idx = min(i + 1000, 8821)\n",
    "        query_ids = df['id'][i:end_idx].values.tolist()\n",
    "        \n",
    "        #Process in parallel\n",
    "        results = Parallel(n_jobs=-1)(\n",
    "            delayed(get_top_similar_documents)(query_id) for query_id in query_ids\n",
    "        )\n",
    "        \n",
    "        temp_df = pd.DataFrame({\n",
    "            \"document_id\": query_ids,\n",
    "            \"similar_document_ids\": results\n",
    "        })\n",
    "        similarity_df = pd.concat([similarity_df, temp_df], ignore_index=True)\n",
    "        \n",
    "        print(f\"Processed and added records from {i} to {end_idx - 1}\")\n",
    "\n",
    "populate_similarity_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similarity_values_exclude_query_with_threshold(start_index=8821, end_index=9354, base_threshold=0.35, top_n=10, min_threshold=0.25):\n",
    "    global similarity_df \n",
    "    results = {}\n",
    "\n",
    "    for i in range(start_index, end_index + 1):\n",
    "        try:\n",
    "            query_vector = np.array(nepberta_array[i])\n",
    "\n",
    "            #Compute NepBERTa similarity\n",
    "            nepberta_sim = cosine_similarity(query_vector.reshape(1, -1), nepberta_array).flatten()\n",
    "            \n",
    "            #Compute BM25 similarity\n",
    "            query_bm25_vector = bm25_vectors_sparse[i]\n",
    "            bm25_sim = query_bm25_vector.dot(bm25_vectors_sparse.T).toarray().flatten()\n",
    "            \n",
    "            #Normalize BM25 to [-1, 1]\n",
    "            bm25_sim_normalized = 2 * (bm25_sim / np.max(bm25_sim)) - 1\n",
    "            \n",
    "            #Ensemble similarity\n",
    "            ensemble_sim = (0.6 * nepberta_sim) + (0.4 * bm25_sim_normalized)\n",
    "            \n",
    "            #Exclude the query index itself\n",
    "            ensemble_sim[i] = -np.inf\n",
    "        \n",
    "            #Apply dynamic threshold adjustment\n",
    "            threshold = base_threshold\n",
    "            adjustment = 0.02\n",
    "            iteration_count = 0\n",
    "            max_iterations = 10\n",
    "\n",
    "            while len(ensemble_sim[ensemble_sim >= threshold]) < 5 and iteration_count < max_iterations:\n",
    "                threshold -= adjustment \n",
    "                \n",
    "                #Ensure threshold doesn't go below the minimum threshold\n",
    "                if threshold < min_threshold:\n",
    "                    threshold = min_threshold\n",
    "                    break\n",
    "                \n",
    "                iteration_count += 1\n",
    "            \n",
    "            #Filter valid indices based on the adjusted threshold\n",
    "            valid_indices = np.where(ensemble_sim >= threshold)[0]\n",
    "\n",
    "            top_indices = valid_indices[np.argsort(-ensemble_sim[valid_indices])]\n",
    "\n",
    "            #Keep only the top N similar documents, or fewer if there aren't enough\n",
    "            if len(top_indices) > top_n:\n",
    "                top_indices = top_indices[:top_n]\n",
    "\n",
    "            #Store the results for valid indices within the threshold\n",
    "            similar_document_ids = [df.iloc[idx][\"id\"] for idx in top_indices if idx != i]  # Exclude the query index itself\n",
    "\n",
    "            #Create a DataFrame with the results to append to similarity_df\n",
    "            temp_df = pd.DataFrame({\n",
    "                \"document_id\": df.iloc[i][\"id\"],  # Return the initial query's index as document_id\n",
    "                \"similar_document_ids\": [similar_document_ids]\n",
    "            })\n",
    "\n",
    "            #Append the temporary DataFrame to the main similarity DataFrame\n",
    "            similarity_df = pd.concat([similarity_df, temp_df], ignore_index=True)\n",
    "\n",
    "            #Save the result for the current document\n",
    "            results[i] = {\n",
    "                \"document_id\": df.iloc[i][\"id\"],  # The query's document ID\n",
    "                \"similar_document_ids\": similar_document_ids,\n",
    "                \"threshold\": threshold,\n",
    "                \"similarities\": ensemble_sim[top_indices]\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing index {i}: {e}\")\n",
    "            results[i] = {\"error\": str(e)}\n",
    "\n",
    "check_similarity_values_exclude_query_with_threshold(8820, 9354)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average length of the lists in the 'similar_document_ids' column is: 6.80\n",
      "Count of documents with no similar documents:  20\n",
      "Indices of documents with no similar documents: [11, 30, 42, 130, 139, 168, 176, 263, 291, 322, 354, 361, 376, 387, 399, 436, 482, 485, 516, 518]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the length of each list in the 'similar_document_ids' column\n",
    "similarity_df['list_length'] = similarity_df['similar_document_ids'].apply(lambda x: len(x) if x else 0)\n",
    "\n",
    "# Calculate the average length of the lists\n",
    "average_length = similarity_df['list_length'].mean()\n",
    "print(f\"The average length of the lists in the 'similar_document_ids' column is: {average_length:.2f}\")\n",
    "\n",
    "# Count the number of rows with empty lists in 'similar_document_ids'\n",
    "empty_list_count = similarity_df['similar_document_ids'].apply(lambda x: isinstance(x, list) and len(x) == 0).sum()\n",
    "print(\"Count of documents with no similar documents: \", empty_list_count)\n",
    "\n",
    "# Get the indices of the rows with no similar documents\n",
    "empty_list_indices = similarity_df[similarity_df['similar_document_ids'].apply(lambda x: isinstance(x, list) and len(x) == 0)].index.tolist()\n",
    "print(f\"Indices of documents with no similar documents: {empty_list_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14204\\1511885449.py:27: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data added to the document_similarities table successfully!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "#Your previous code\n",
    "base_path = Path.cwd() \n",
    "config_path = base_path / \"config.ini\"\n",
    "\n",
    "config = ConfigParser()\n",
    "config.read(config_path)\n",
    "\n",
    "#Access database details\n",
    "db_config = config['database']\n",
    "username = db_config['username']\n",
    "pwd = db_config['password']\n",
    "hostname = db_config['hostname']\n",
    "port_id = int(db_config['port_id'])\n",
    "database = db_config['database']\n",
    "\n",
    "#Create engine\n",
    "engine = create_engine(f'postgresql://{username}:{pwd}@{hostname}:{port_id}/{database}')\n",
    "\n",
    "#Create session\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "#Define ORM model\n",
    "Base = declarative_base()\n",
    "\n",
    "class Results(Base):\n",
    "    __tablename__ = \"document_similarities\"\n",
    "\n",
    "    document_id = Column(String, primary_key=True)\n",
    "    similar_document_ids = Column(String)\n",
    "\n",
    "#Assuming you have a DataFrame `similarity_df` loaded\n",
    "similarity_df = similarity_df[[\"document_id\", \"similar_document_ids\"]]\n",
    "\n",
    "#Loop through rows and save to the database\n",
    "for _, row in similarity_df.iterrows():\n",
    "    similar_document_ids = row[\"similar_document_ids\"]\n",
    "\n",
    "    similar_document_ids = json.dumps([similar_document_ids])\n",
    "\n",
    "    #Check if the document_id already exists\n",
    "    existing_row = session.query(Results).filter_by(document_id=row[\"document_id\"]).first()\n",
    "    if existing_row:\n",
    "        existing_row.similar_document_ids = similar_document_ids\n",
    "    else:\n",
    "        new_row = Results(\n",
    "            document_id=row[\"document_id\"],\n",
    "            similar_document_ids=similar_document_ids\n",
    "        )\n",
    "        session.add(new_row)\n",
    "\n",
    "#Commit the changes\n",
    "session.commit()\n",
    "\n",
    "print(\"Data added to the document_similarities table successfully!\")\n",
    "\n",
    "#Close the session\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
