{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "from pathlib import Path\n",
    "\n",
    "#Define relative path\n",
    "base_path = Path.cwd()\n",
    "config_path = base_path /\"config_vector.ini\"\n",
    "\n",
    "#Initialize config parser\n",
    "config = ConfigParser()\n",
    "config.read(config_path)\n",
    "\n",
    "#Accessing database details\n",
    "db_config = config['database']\n",
    "username = db_config['username']\n",
    "pwd = db_config['password']\n",
    "hostname = db_config['hostname']\n",
    "port_id = int(db_config['port_id'])\n",
    "database = db_config['database']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14000\\1808304932.py:13: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>data</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>१००३८</td>\n",
       "      <td>&lt;div class=\"col-md-8 para-sections\"&gt;\\n&lt;div id=...</td>\n",
       "      <td>{'body': ['न्या.डा.आनन्दमोहन भट्टराई : न्याय प...</td>\n",
       "      <td>२०७४-१२-१९</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>४६१५</td>\n",
       "      <td>&lt;div class=\"col-md-8 para-sections\"&gt;\\n&lt;div id=...</td>\n",
       "      <td>{'body': ['न्या.गजेन्द्रकेशरी वास्तोला', '१.  ...</td>\n",
       "      <td>२०४९-०९-०८</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>११२१</td>\n",
       "      <td>&lt;div class=\"col-md-8 para-sections\"&gt;\\n&lt;div id=...</td>\n",
       "      <td>{'body': ['न्या. धनेन्द्रबहादुर सिंह : प्रस्तु...</td>\n",
       "      <td>२०३५-०१-१२</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>२४०६</td>\n",
       "      <td>&lt;div class=\"col-md-8 para-sections\"&gt;\\n&lt;div id=...</td>\n",
       "      <td>{'body': ['न्या.बब्बरप्रसाद सिंहः नेपालको संवि...</td>\n",
       "      <td>२०४२-०५-१९</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>३१४४</td>\n",
       "      <td>&lt;div class=\"col-md-8 para-sections\"&gt;\\n&lt;div id=...</td>\n",
       "      <td>{'body': ['न्या.पृथ्वी बहादुर सिंहः नेपालको सं...</td>\n",
       "      <td>२०४४-०२-२५</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                            content  \\\n",
       "0  १००३८  <div class=\"col-md-8 para-sections\">\\n<div id=...   \n",
       "1   ४६१५  <div class=\"col-md-8 para-sections\">\\n<div id=...   \n",
       "2   ११२१  <div class=\"col-md-8 para-sections\">\\n<div id=...   \n",
       "3   २४०६  <div class=\"col-md-8 para-sections\">\\n<div id=...   \n",
       "4   ३१४४  <div class=\"col-md-8 para-sections\">\\n<div id=...   \n",
       "\n",
       "                                                data        date  \n",
       "0  {'body': ['न्या.डा.आनन्दमोहन भट्टराई : न्याय प...  २०७४-१२-१९  \n",
       "1  {'body': ['न्या.गजेन्द्रकेशरी वास्तोला', '१.  ...  २०४९-०९-०८  \n",
       "2  {'body': ['न्या. धनेन्द्रबहादुर सिंह : प्रस्तु...  २०३५-०१-१२  \n",
       "3  {'body': ['न्या.बब्बरप्रसाद सिंहः नेपालको संवि...  २०४२-०५-१९  \n",
       "4  {'body': ['न्या.पृथ्वी बहादुर सिंहः नेपालको सं...  २०४४-०२-२५  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating SQLAlchemy engine\n",
    "from sqlalchemy import create_engine, Column, String\n",
    "engine = create_engine(f'postgresql://{username}:{pwd}@{hostname}:{port_id}/{database}')\n",
    "\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "#Creating a session\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "#Defining ORM model\n",
    "Base = declarative_base()\n",
    "\n",
    "class Najirs(Base):\n",
    "    __tablename__ = \"najirs_v4\"\n",
    "\n",
    "    id = Column(String, primary_key=True)\n",
    "    content = Column(String)\n",
    "    date = Column(String)\n",
    "    data = Column(String)\n",
    "\n",
    "#Query data safely\n",
    "results = session.query(Najirs.id, Najirs.content, Najirs.data, Najirs.date).all()\n",
    "df = pd.DataFrame(results, columns= [\"id\", \"content\", \"data\", \"date\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 572 rows with empty 'principles' key.\n"
     ]
    }
   ],
   "source": [
    "df.drop(['id', 'content', 'date'], axis = 'columns', inplace = True)\n",
    "\n",
    "def is_principles_empty(content):\n",
    "    return 'principles' not in content or not content['principles']\n",
    "\n",
    "#Removing rows with empty 'principles'\n",
    "empty_principles_mask = df['data'].apply(is_principles_empty)\n",
    "empty_principles_count = empty_principles_mask.sum()\n",
    "df = df[~empty_principles_mask]\n",
    "print(f\"Removed {empty_principles_count} rows with empty 'principles' key.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = base_path/\"nepali_stopwords.txt\"\n",
    "\n",
    "#Reading the content of the file\n",
    "with file_path.open(encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "stopwords = [word.strip().strip('\"') for word in content.split(\",\")]\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = re.sub(r'[\\u200c\\u200d]', '', text)  #Remove unwanted characters like \\u200c and \\u200d\n",
    "    text = re.sub(r'[^\\u0900-\\u097F\\s]', '', text)  #Keep only Devanagari characters and spaces\n",
    "    text = re.sub(r'\\d+', '', text)  #Remove digits\n",
    "\n",
    "    #Tokenize and remove stopwords\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stopwords]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Extract and preprocess \"subject\" and \"principles\"\n",
    "def extract_text(content):\n",
    "    try:\n",
    "        text = \"\"\n",
    "        for key, value in content.items():\n",
    "            if key in [\"subject\", \"principles\"]:\n",
    "                if isinstance(value, str):\n",
    "                    text += value + \" \"\n",
    "                elif isinstance(value, list):\n",
    "                    for sublist in value:\n",
    "                        if isinstance(sublist, list):\n",
    "                            text += \" \".join(sublist) + \" \"\n",
    "                        else:\n",
    "                            text += sublist + \" \"\n",
    "        return clean_text(text.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing content: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[\\u200c\\u200d]', '', text)\n",
    "    text = re.sub(r'[^\\u0900-\\u097F\\s]', '', text)  #Keep only Devanagari characters and spaces\n",
    "    text = re.sub(r'\\d+', '', text)  #Remove digits\n",
    "    text = re.sub(r'।', '', text)\n",
    "\n",
    "    tokens = text.split()\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def extract_text(content):\n",
    "    try:\n",
    "        text = []\n",
    "        for key, value in content.items():\n",
    "            if key in [\"subject\", \"principles\"]:\n",
    "                if isinstance(value, str):\n",
    "                    text.append(value)\n",
    "                elif isinstance(value, list):\n",
    "                    for sublist in value:\n",
    "                        if isinstance(sublist, list):\n",
    "                            text.append(\" \".join(map(str, sublist)))\n",
    "                        else:\n",
    "                            text.append(str(sublist))\n",
    "        combined_text = \" \".join(text)\n",
    "        return clean_text(combined_text.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing content: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying preprocessing\n",
    "df['cleaned_text'] = df['data'].apply(extract_text)\n",
    "df.drop(['data'], axis = 'columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and Fine-Tuning The NepBERTa Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#Loading NepBERTa tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"NepBERTa/NepBERTa\")\n",
    "model = BertModel.from_pretrained(\"NepBERTa/NepBERTa\", from_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding done\n"
     ]
    }
   ],
   "source": [
    "def get_sentence_embedding(sentences):\n",
    "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.pooler_output.numpy()\n",
    "\n",
    "#Generating embeddings\n",
    "df['embeddings'] = df['cleaned_text'].apply(lambda x: get_sentence_embedding([x]) if x else None)\n",
    "print(\"Embedding done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [[-0.023295283, 0.04019555, 0.22646093, 0.1248...\n",
      "1    [[-0.18046029, 0.052735027, 0.23040603, 0.1659...\n",
      "2    [[-0.2502869, -0.0183751, 0.117184184, 0.09710...\n",
      "3    [[-0.2658006, -0.10030958, 0.21205233, 0.08202...\n",
      "4    [[-0.11624412, -0.095075056, 0.28900784, 0.041...\n",
      "Name: embeddings, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['embeddings'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export path: d:\\Python\\ML-Projects\\Intern-Synapse\\TextVectorization\\najirs_NepBERTa.pkl\n",
      "Data Frame Saved!\n"
     ]
    }
   ],
   "source": [
    "#Full file path for saving the DataFrame\n",
    "export_path = base_path / \"najirs_NepBERTa.pkl\"\n",
    "\n",
    "#Saving the DataFrame\n",
    "df.to_pickle(export_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
